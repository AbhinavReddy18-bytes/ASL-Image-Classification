{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f382b2c6-577d-4f91-b1dc-d211bbc657b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "212/212 [==============================] - 12s 52ms/step - loss: 5.2616 - accuracy: 0.1220 - val_loss: 6.0595 - val_accuracy: 0.1113 - lr: 3.0000e-04\n",
      "Epoch 2/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 4.6421 - accuracy: 0.1334 - val_loss: 4.8237 - val_accuracy: 0.1113 - lr: 3.0000e-04\n",
      "Epoch 3/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 4.3487 - accuracy: 0.1516 - val_loss: 4.1800 - val_accuracy: 0.1551 - lr: 3.0000e-04\n",
      "Epoch 4/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 4.0602 - accuracy: 0.1697 - val_loss: 3.8687 - val_accuracy: 0.2362 - lr: 3.0000e-04\n",
      "Epoch 5/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 3.7566 - accuracy: 0.2039 - val_loss: 3.7514 - val_accuracy: 0.1652 - lr: 3.0000e-04\n",
      "Epoch 6/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 3.4658 - accuracy: 0.2431 - val_loss: 3.3578 - val_accuracy: 0.2410 - lr: 3.0000e-04\n",
      "Epoch 7/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 3.1677 - accuracy: 0.2936 - val_loss: 3.0798 - val_accuracy: 0.3102 - lr: 3.0000e-04\n",
      "Epoch 8/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 2.8778 - accuracy: 0.3413 - val_loss: 2.9942 - val_accuracy: 0.2747 - lr: 3.0000e-04\n",
      "Epoch 9/60\n",
      "212/212 [==============================] - 11s 51ms/step - loss: 2.6133 - accuracy: 0.4058 - val_loss: 2.3408 - val_accuracy: 0.5145 - lr: 3.0000e-04\n",
      "Epoch 10/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 2.3825 - accuracy: 0.4563 - val_loss: 2.2402 - val_accuracy: 0.5631 - lr: 3.0000e-04\n",
      "Epoch 11/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 2.1965 - accuracy: 0.5102 - val_loss: 2.0304 - val_accuracy: 0.5583 - lr: 3.0000e-04\n",
      "Epoch 12/60\n",
      "212/212 [==============================] - 11s 51ms/step - loss: 2.0270 - accuracy: 0.5548 - val_loss: 1.8586 - val_accuracy: 0.6264 - lr: 3.0000e-04\n",
      "Epoch 13/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.9823 - accuracy: 0.5791 - val_loss: 1.8519 - val_accuracy: 0.6128 - lr: 3.0000e-04\n",
      "Epoch 14/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.8481 - accuracy: 0.6183 - val_loss: 1.8244 - val_accuracy: 0.6507 - lr: 3.0000e-04\n",
      "Epoch 15/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.7489 - accuracy: 0.6515 - val_loss: 1.7540 - val_accuracy: 0.6696 - lr: 3.0000e-04\n",
      "Epoch 16/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.6674 - accuracy: 0.6846 - val_loss: 1.7666 - val_accuracy: 0.6459 - lr: 3.0000e-04\n",
      "Epoch 17/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.6143 - accuracy: 0.6980 - val_loss: 1.5134 - val_accuracy: 0.7217 - lr: 3.0000e-04\n",
      "Epoch 18/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.5818 - accuracy: 0.7154 - val_loss: 1.5670 - val_accuracy: 0.7294 - lr: 3.0000e-04\n",
      "Epoch 19/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.5035 - accuracy: 0.7384 - val_loss: 1.3460 - val_accuracy: 0.8046 - lr: 3.0000e-04\n",
      "Epoch 20/60\n",
      "212/212 [==============================] - 11s 51ms/step - loss: 1.4508 - accuracy: 0.7572 - val_loss: 1.3157 - val_accuracy: 0.8022 - lr: 3.0000e-04\n",
      "Epoch 21/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.4308 - accuracy: 0.7640 - val_loss: 1.6047 - val_accuracy: 0.7235 - lr: 3.0000e-04\n",
      "Epoch 22/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.3760 - accuracy: 0.7760 - val_loss: 1.3597 - val_accuracy: 0.7993 - lr: 3.0000e-04\n",
      "Epoch 23/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.3248 - accuracy: 0.7936 - val_loss: 1.2839 - val_accuracy: 0.8058 - lr: 3.0000e-04\n",
      "Epoch 24/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.3269 - accuracy: 0.7948 - val_loss: 1.8637 - val_accuracy: 0.5986 - lr: 3.0000e-04\n",
      "Epoch 25/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.3179 - accuracy: 0.8016 - val_loss: 1.4266 - val_accuracy: 0.7880 - lr: 3.0000e-04\n",
      "Epoch 26/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.2873 - accuracy: 0.8035 - val_loss: 1.6391 - val_accuracy: 0.6951 - lr: 3.0000e-04\n",
      "Epoch 27/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.2651 - accuracy: 0.8117 - val_loss: 1.1491 - val_accuracy: 0.8632 - lr: 3.0000e-04\n",
      "Epoch 28/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.2296 - accuracy: 0.8244 - val_loss: 1.1802 - val_accuracy: 0.8289 - lr: 3.0000e-04\n",
      "Epoch 29/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.2015 - accuracy: 0.8333 - val_loss: 1.0767 - val_accuracy: 0.8727 - lr: 3.0000e-04\n",
      "Epoch 30/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.1904 - accuracy: 0.8297 - val_loss: 1.1349 - val_accuracy: 0.8514 - lr: 3.0000e-04\n",
      "Epoch 31/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.1263 - accuracy: 0.8519 - val_loss: 1.3298 - val_accuracy: 0.7809 - lr: 3.0000e-04\n",
      "Epoch 32/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.1517 - accuracy: 0.8342 - val_loss: 1.1861 - val_accuracy: 0.8336 - lr: 3.0000e-04\n",
      "Epoch 33/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.1115 - accuracy: 0.8496 - val_loss: 1.1264 - val_accuracy: 0.8425 - lr: 3.0000e-04\n",
      "Epoch 34/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0904 - accuracy: 0.8580 - val_loss: 1.1147 - val_accuracy: 0.8455 - lr: 3.0000e-04\n",
      "Epoch 35/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.0683 - accuracy: 0.8592 - val_loss: 1.0236 - val_accuracy: 0.8769 - lr: 3.0000e-04\n",
      "Epoch 36/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0630 - accuracy: 0.8579 - val_loss: 1.0774 - val_accuracy: 0.8555 - lr: 3.0000e-04\n",
      "Epoch 37/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.0485 - accuracy: 0.8617 - val_loss: 1.0362 - val_accuracy: 0.8733 - lr: 3.0000e-04\n",
      "Epoch 38/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0220 - accuracy: 0.8707 - val_loss: 4.6540 - val_accuracy: 0.3102 - lr: 3.0000e-04\n",
      "Epoch 39/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0268 - accuracy: 0.8715 - val_loss: 1.1129 - val_accuracy: 0.8591 - lr: 3.0000e-04\n",
      "Epoch 40/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 1.0137 - accuracy: 0.8724 - val_loss: 1.0895 - val_accuracy: 0.8496 - lr: 3.0000e-04\n",
      "Epoch 41/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.9910 - accuracy: 0.8798 - val_loss: 0.9729 - val_accuracy: 0.8893 - lr: 3.0000e-04\n",
      "Epoch 42/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0171 - accuracy: 0.8736 - val_loss: 6.2417 - val_accuracy: 0.1208 - lr: 3.0000e-04\n",
      "Epoch 43/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 0.9588 - accuracy: 0.8867 - val_loss: 2.0139 - val_accuracy: 0.5204 - lr: 3.0000e-04\n",
      "Epoch 44/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 1.0101 - accuracy: 0.8724 - val_loss: 1.0549 - val_accuracy: 0.8680 - lr: 3.0000e-04\n",
      "Epoch 45/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.9778 - accuracy: 0.8746 - val_loss: 5.4293 - val_accuracy: 0.1658 - lr: 3.0000e-04\n",
      "Epoch 46/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 0.9600 - accuracy: 0.8793 - val_loss: 1.0097 - val_accuracy: 0.8739 - lr: 3.0000e-04\n",
      "Epoch 47/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 0.9565 - accuracy: 0.8838 - val_loss: 4.6187 - val_accuracy: 0.1308 - lr: 3.0000e-04\n",
      "Epoch 48/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 0.9330 - accuracy: 0.8857 - val_loss: 1.2865 - val_accuracy: 0.7762 - lr: 3.0000e-04\n",
      "Epoch 49/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.8760 - accuracy: 0.9045 - val_loss: 0.8497 - val_accuracy: 0.9207 - lr: 3.0000e-05\n",
      "Epoch 50/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.8383 - accuracy: 0.9152 - val_loss: 0.8094 - val_accuracy: 0.9325 - lr: 3.0000e-05\n",
      "Epoch 51/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.7968 - accuracy: 0.9218 - val_loss: 0.7859 - val_accuracy: 0.9307 - lr: 3.0000e-05\n",
      "Epoch 52/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.7874 - accuracy: 0.9227 - val_loss: 0.7644 - val_accuracy: 0.9361 - lr: 3.0000e-05\n",
      "Epoch 53/60\n",
      "212/212 [==============================] - 10s 49ms/step - loss: 0.7541 - accuracy: 0.9286 - val_loss: 0.7745 - val_accuracy: 0.9278 - lr: 3.0000e-05\n",
      "Epoch 54/60\n",
      "212/212 [==============================] - 11s 51ms/step - loss: 0.7408 - accuracy: 0.9329 - val_loss: 0.7408 - val_accuracy: 0.9355 - lr: 3.0000e-05\n",
      "Epoch 55/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.7208 - accuracy: 0.9335 - val_loss: 0.7263 - val_accuracy: 0.9384 - lr: 3.0000e-05\n",
      "Epoch 56/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.6948 - accuracy: 0.9350 - val_loss: 0.7205 - val_accuracy: 0.9366 - lr: 3.0000e-05\n",
      "Epoch 57/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.6939 - accuracy: 0.9357 - val_loss: 0.7146 - val_accuracy: 0.9372 - lr: 3.0000e-05\n",
      "Epoch 58/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.6878 - accuracy: 0.9372 - val_loss: 0.6895 - val_accuracy: 0.9384 - lr: 3.0000e-05\n",
      "Epoch 59/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.6715 - accuracy: 0.9346 - val_loss: 0.6892 - val_accuracy: 0.9390 - lr: 3.0000e-05\n",
      "Epoch 60/60\n",
      "212/212 [==============================] - 11s 50ms/step - loss: 0.6571 - accuracy: 0.9350 - val_loss: 0.6829 - val_accuracy: 0.9337 - lr: 3.0000e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tough\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def loadingData():\n",
    "    data_full = np.load('data_train.npy')\n",
    "    labels_all = np.load('labels_train.npy')\n",
    "\n",
    "    #Reshape and Resize data\n",
    "    reshaped_data = data.T.reshape((-1, 300, 300, 3))\n",
    "    resizing_data = np.array([resize(img, (100, 100, 3), anti_aliasing=True) for img in reshaped_data])\n",
    "\n",
    "    # Preprocessing the data\n",
    "    normalizingData = resizing_data / 255.0\n",
    "\n",
    "    return normalizingData, labels_all\n",
    "\n",
    "def building_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(512, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        #Increasing Complexity of the model and adjusting dropout rate with more Dense\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.4),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.3),\n",
    "        Dense(9, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def trainingSet():\n",
    "    data_full, labels_all = loadingData()\n",
    "\n",
    "    # Using Data augmentation generator and increasing it.\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Splitting the data into 80% of Training data and 20% Testing data.\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(data_full, labels_all, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    model = building_model()\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0003), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks which keeps an eye on validation_loss:\n",
    "    earlyStopping = tough.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "    learningRate_scheduler = tough.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, min_lr=0.00001)\n",
    "    modellingCheckpoint = tough.keras.callbacks.ModelCheckpoint('my_best_model.h5', save_best_only=True)\n",
    "\n",
    "    # Training the model with the parameters given in the callbacks functions.\n",
    "    trainingGenerator = datagen.flow(train_data, train_labels, batch_size=32)\n",
    "    model.fit(trainingGenerator, epochs=60, validation_data=(val_data, val_labels),\n",
    "              callbacks=[earlyStopping, learningRate_scheduler, modellingCheckpoint])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainingSet()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
